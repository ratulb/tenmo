# ðŸ§  Mojo Tensor(tenmo)

A blazing-fast, from-scratch **Tensor** library built in [MojoðŸ”¥](https://modular.com/mojo), with full support for:

> âš ï¸ **Warning:** This library is in **rapid progress**.  
> The API is evolving and may break between Mojo versions.  
> If you encounter issues, please consult the issue tracker and feel free to report or submit a PR.

- ðŸ§® N-dimensional Tensors
- ðŸ” Broadcasting
- ðŸ”¢ Automatic differentiation
- ðŸ§  Scalar & elementwise operations
- ðŸ§¬ SIMD vectorization
- ðŸªœ Views and slicing
- ðŸ§ª Comprehensive test coverage

> ðŸš§ **Work in Progress** â€“ This project is evolving as Mojo is. Breaking changes are to be expected. Built with â¤ï¸ from scratch â€” no NumPy, PyTorch, or TensorFlow under the hood.

---

## ðŸš€ Features

| Feature                  | Status | Notes |
|--------------------------|--------|-------|
| âœ… N-dimensional Tensor   | âœ”ï¸     | `Tensor[DType.float64]`, `Tensor[DType.bool]`, etc|
| âœ… Manual Broadcasting    | âœ”ï¸     | Compatible shapes |
| âœ… Elementwise Ops        | âœ”ï¸     | `+`, `-`, `*`, `/`, `pow`, etc. |
| âœ… SIMD Support           | âœ”ï¸     | Vectorized compute for speed |
| âœ… Pretty-printing        | âœ”ï¸     | Recursive + aligned |
| âœ… Slicing & Views        | âœ”ï¸     | Offset-aware `TensorView` |
| âœ… Autodiff               | ðŸ§ª     | Scalar ops + graph-based backprop |
| âœ… Gradient Tracking      | ðŸ§ª     | In progress full swing |
| âœ… Unit Testing           | âœ”ï¸     | Custom test suite |

---

## ðŸ“¦ Example

```mojo
from tensors import Tensor

Tensor.arange(6, requires_grad=True).reshape(2, 3).print()

[2D Tensor(2, 3), Type: float32, requires_grad: True]
  [
    [0.0, 1.0, 2.0, ],
    [3.0, 4.0, 5.0, ],
  ]


# Broadcasting + elementwise op
t2 = t + 1.0
t2.print()

# Gradient tracking
y = t * 2.0
y.backward()
t.grad[].print() or t.gprint()

a = Tensor.arange(12, requires_grad=True)
v1 = a.view([5, 2], offset=2)   # Slice starting from a[2]
v2 = v1.view([2, 5])            # Reshape
v3 = v2.view([10])              # Flatten

v3.backward()
expected = Tensor.ones(12)
expected[0] = 0
expected[1] = 0
assert_true((a.grad[] == expected).all_true())
v3.free()
v2.free()
v1.free()
a.free()

```

---

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ tensors.mojo                # Core Tensor implementation
â”œâ”€â”€ views.mojo                  # TensorView (slicing/view logic)
â”œâ”€â”€ shapes.mojo                 # Shape logic and utilities
â”œâ”€â”€ intlist.mojo                # Light Intger list backing many operations
â”œâ”€â”€ operators.mojo              # Vectorized ops
â”œâ”€â”€ tests/test_tensors.mojo     # Unit tests
â””â”€â”€ README.md                   # You're here!
```

---

## ðŸ§ª Running Tests

Run using the Mojo CLI:

```bash
./execute tensors
```

Tests cover:
- Shape + flattening
- Broadcasting correctness
- Slicing and views

---

## ðŸ”¬ Why Mojo?

Mojo combines Pythonâ€™s usability with Câ€™s performance and MLIRâ€™s power. This project explores:

- Low-level memory and layout control
- SIMD vectorization
- Manual shape + broadcast logic
- First-principles autodiff 
- Efficient slicing and in-place views

Perfect for understanding what deep learning libraries **actually do under the hood**.

---

## ðŸ› ï¸ Roadmap

- [x] Optimized matmul/optmization in general
- [x] Transperant GPU support
- [x] Distributed Training


---

## ðŸ’¡ Inspirations

- [NumPy](https://numpy.org/)  
- [PyTorch internals](https://pytorch.org/)  
- [Karpathy's `llm.c`](https://github.com/karpathy/llm.c)  
- [Mojo by Modular](https://www.modular.com/mojo)  

---

## ðŸ§  Philosophy

> Build from scratch. Understand deeply. Control everything. Optimize aggressively.

This is a **learning-first** project: correctness first, performance next, scale later.

---

## ðŸ“œ License

MIT License â€“ do what you love. Attribution appreciated, not required.

---


