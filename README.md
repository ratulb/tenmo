# ğŸ§  Mojo Tensor

A blazing-fast, from-scratch **Tensor** library built in [MojoğŸ”¥](https://modular.com/mojo), with full support for:

- ğŸ§® N-dimensional Tensors
- ğŸ” Broadcasting
- ğŸ”¢ Autodiff (automatic differentiation)
- ğŸ§  Scalar & elementwise operations
- ğŸ§¬ SIMD vectorization
- ğŸªœ Views and slicing
- ğŸ§ª Comprehensive test coverage

> ğŸš§ **Work in Progress** â€“ This project is a deep dive into Mojo internals, systems programming, and the foundation of modern deep learning libraries. Built with â¤ï¸ from scratch â€” no NumPy, PyTorch, or TensorFlow under the hood.

---

## ğŸš€ Features

| Feature                  | Status | Notes |
|--------------------------|--------|-------|
| âœ… N-dimensional Tensor   | âœ”ï¸     | `Tensor[DType.float64]`, `Tensor[DType.bool]`, etc|
| âœ… Manual Broadcasting    | âœ”ï¸     | Compatible shapes |
| âœ… Elementwise Ops        | âœ”ï¸     | `+`, `-`, `*`, `/`, `pow`, etc. |
| âœ… SIMD Support           | âœ”ï¸     | Vectorized compute for speed |
| âœ… Pretty-printing        | âœ”ï¸     | Recursive + aligned |
| âœ… Slicing & Views        | âœ”ï¸     | Offset-aware `TensorView` |
| âœ… Autodiff               | ğŸ§ª     | Scalar ops + graph-based backprop |
| âœ… Gradient Tracking      | ğŸ§ª     | In progress |
| âœ… Unit Testing           | âœ”ï¸     | Custom test suite |

---

## ğŸ“¦ Example

```mojo
from tensors import Tensor

Tensor.arange(6, requires_grad=True).reshape(2, 3).print()

[2D Tensor(2, 3), Type: float32, requires_grad: True]
  [
    [0.0, 1.0, 2.0, ],
    [3.0, 4.0, 5.0, ],
  ]


# Broadcasting + elementwise op
t2 = t + 1.0
t2.print()

# Gradient tracking
y = t * 2.0
y.backward()
t.grad[].print()
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ tensors.mojo                # Core Tensor implementation
â”œâ”€â”€ views.mojo                  # TensorView (slicing/view logic)
â”œâ”€â”€ shapes.mojo                 # Shape logic and utilities
â”œâ”€â”€ intlist.mojo                # Light Intger list backing many operations
â”œâ”€â”€ operators.mojo              # Vectorized ops
â”œâ”€â”€ tests/test_tensors.mojo     # Unit tests
â””â”€â”€ README.md                   # You're here!
```

---

## ğŸ§ª Running Tests

Run using the Mojo CLI:

```bash
./execute tensors
```

Tests cover:
- Shape + flattening
- Broadcasting correctness
- Slicing and views

---

## ğŸ”¬ Why Mojo?

Mojo combines Pythonâ€™s usability with Câ€™s performance and MLIRâ€™s power. This project explores:

- Low-level memory and layout control
- SIMD vectorization
- Manual shape + broadcast logic
- First-principles autodiff 
- Efficient slicing and in-place views

Perfect for understanding what deep learning libraries **actually do under the hood**.

---

## ğŸ› ï¸ Roadmap

- [x] Optimized matmul/optmization in general
- [x] Transperant GPU support
- [x] Distributed Training


---

## ğŸ’¡ Inspirations

- [NumPy](https://numpy.org/)  
- [PyTorch internals](https://pytorch.org/)  
- [Karpathy's `llm.c`](https://github.com/karpathy/llm.c)  
- [Mojo by Modular](https://www.modular.com/mojo)  

---

## ğŸ§  Philosophy

> Build from scratch. Understand deeply. Control everything. Optimize aggressively.

This is a **learning-first** project: correctness first, performance next, scale later.

---

## ğŸ“œ License

MIT License â€“ do what you love. Attribution appreciated, not required.

---

## ğŸ‘‹ Author

**Ratul Buragohain** â€” Machine learning enthusiast, and autodiff explorer ğŸ

---

